# RAG-fromScratch

ğŸ‘¨ğŸ»â€ğŸ’» The Python notebook contains both code and notes produced during <a href="https://youtu.be/qN_2fnOPY-M?si=nhuguHBT04HxDOfc">this</a> Daniel Bourke YouTube lesson. The main focus here was *not* to produce an optimized and scalable RAG pipeline, but rather to build intuition behind the core RAG processâ€”without using frameworks such as LangChain or LlamaIndex, just Python with minimal tools.

By building this RAG pipeline from scratch, I was able to work with PDF loading and preprocessing, embedding models, similarity search, running an LLM locally, and prompt augmentation.

Key test-and-try points learned that are crucial for a good RAG pipeline:

- Choose a good embedding model and check if it was trained in the desired language(s). Also, check if a small embedding model works well, as it results in a faster pipeline and less storage.  
- If your RAG pipeline is mainly to summarize and structure the relevant pieces of text, you definitely donâ€™t need a big modelâ€”start with a small one (< 10B).  
- Writing a good final prompt is a crucial part (some useful resources: <a href="https://www.promptingguide.ai/pt/introduction/examples">Prompting Guide</a>, <a href="https://github.com/brexhq/prompt-engineering">Brexâ€™s Prompt Engineering Guide</a>, <a href="https://arxiv.org/abs/2401.14423">Related arXiv Paper</a>).  

Feel free to check out the notebook!
